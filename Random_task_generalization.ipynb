{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random_task_generalization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Kh57cZ7h5W7jmmWrSJV_uEcXVemtxbwo",
      "authorship_tag": "ABX9TyNG2E0u3NnSMuiIcbhISMg4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minddo/Personalized_emotion_recognition/blob/main/Random_task_generalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGYHPEkuV-1k"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Q6ddVdViSW"
      },
      "source": [
        "import pickle\r\n",
        "import numpy as np\r\n",
        "dataset=pickle.load(open('/content/drive/My Drive/KEMOC/dataset_30.pkl', 'rb'), encoding='iso-8859-1')\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBdD2NaEXFza"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "\r\n",
        "# LOSO를 위한 데이터셋 저장\r\n",
        "LOSO_dataset={}\r\n",
        "\r\n",
        "removed_dataset={}\r\n",
        "\r\n",
        "for ID in dataset.keys():\r\n",
        "    if ID=='p22' or ID=='p27' or ID=='p8' or ID=='p26' or ID=='p32':\r\n",
        "        continue\r\n",
        "    else:\r\n",
        "        removed_dataset[ID]={\"data\":dataset[ID]['data'], \"labels\":dataset[ID]['labels']}\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "\r\n",
        "# LOSO를 수행하기 위한 데이터셋 저장\r\n",
        "\r\n",
        "LOSO_dataset={}\r\n",
        "for ID in removed_dataset.keys():\r\n",
        "    count=0\r\n",
        "    for ID2 in removed_dataset.keys():\r\n",
        "        if ID==ID2:\r\n",
        "            X=np.concatenate((np.array(removed_dataset[ID]['data']['bvp']).reshape(-1,120,1),np.array(removed_dataset[ID]['data']['eda']).reshape(-1,120,1), np.array(removed_dataset[ID]['data']['temp']).reshape(-1,120,1), np.array(removed_dataset[ID]['data']['ecg']).reshape(-1,120,1)), axis=2)\r\n",
        "            Y=dataset[ID]['labels']['self_arousal']\r\n",
        "            Y=np.array([item[0] for item in Y])\r\n",
        "            \r\n",
        "            X=torch.Tensor(X)\r\n",
        "            Y=torch.Tensor(Y).long()\r\n",
        "\r\n",
        "            test_data=TensorDataset(X,Y)\r\n",
        "        elif ID!=ID2:\r\n",
        "            X=np.concatenate((np.array(removed_dataset[ID2]['data']['bvp']).reshape(-1,120,1),np.array(removed_dataset[ID2]['data']['eda']).reshape(-1,120,1), np.array(removed_dataset[ID2]['data']['temp']).reshape(-1,120,1), np.array(removed_dataset[ID2]['data']['ecg']).reshape(-1,120,1)), axis=2)\r\n",
        "            Y=dataset[ID2]['labels']['self_arousal']\r\n",
        "            Y=np.array([item[0] for item in Y])\r\n",
        "            \r\n",
        "            if count==0:\r\n",
        "                data=X\r\n",
        "                labels=Y\r\n",
        "                count+=1\r\n",
        "            else:\r\n",
        "                data=np.concatenate((data,X), axis=0)\r\n",
        "                labels=np.concatenate((labels,Y), axis=0)\r\n",
        "    data=torch.Tensor(data)\r\n",
        "    labels=torch.Tensor(labels).long()\r\n",
        "    train_data=TensorDataset(data,labels)\r\n",
        "    LOSO_dataset[ID]={\"train\": train_data, 'test':test_data}\r\n",
        "                \r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scx6SEujWCEX"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssE9nV7EWEN9"
      },
      "source": [
        "## Hyper parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPF1x4yccHRN"
      },
      "source": [
        "!pip install learn2learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tSB73HaXLeb"
      },
      "source": [
        "import learn2learn as l2l\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision.datasets import MNIST\r\n",
        "import torchvision"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2ANQgtyXOBP"
      },
      "source": [
        "# Device configuration\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "# Hyper-parameters\r\n",
        "sequence_length = 120\r\n",
        "input_size = 4\r\n",
        "hidden_size = 32\r\n",
        "num_layers = 2\r\n",
        "num_classes = 2\r\n",
        "\r\n",
        "lr=0.05\r\n",
        "maml_lr=0.05 \r\n",
        "iterations=500\r\n",
        "ways=2\r\n",
        "shots=2\r\n",
        "tps=16\r\n",
        "fas=5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfc3Z_olWHRo"
      },
      "source": [
        "##Model and metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooNoBmjEWK2n"
      },
      "source": [
        "class BiRNN(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\r\n",
        "        super(BiRNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\r\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        # Set initial states\r\n",
        "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \r\n",
        "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\r\n",
        "        \r\n",
        "        # Forward propagate LSTM\r\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\r\n",
        "        \r\n",
        "        # Decode the hidden state of the last time step\r\n",
        "        out = self.fc(out[:, -1, :])\r\n",
        "        return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfv9LgZjXS3m"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFMYGU5JX7Gh",
        "outputId": "c955ca80-f34e-4761-f999-2c46f6eea8b6"
      },
      "source": [
        "len(LOSO_dataset.keys())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwfBeE3lWQAW"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5In9ndtAWQ5C",
        "outputId": "090be32d-e092-4fa6-9164-f062831c5442"
      },
      "source": [
        "iterations=500\r\n",
        "for ID in LOSO_dataset.keys():\r\n",
        "    training_data=LOSO_dataset[ID]['train']\r\n",
        "    meta_data = l2l.data.MetaDataset(training_data)\r\n",
        "    train_tasks = l2l.data.TaskDataset(meta_data,\r\n",
        "                                        task_transforms=[\r\n",
        "                                                l2l.data.transforms.NWays(meta_data, ways),\r\n",
        "                                                l2l.data.transforms.KShots(meta_data, 2*shots),\r\n",
        "                                                l2l.data.transforms.LoadData(meta_data),\r\n",
        "                                                l2l.data.transforms.RemapLabels(meta_data),\r\n",
        "                                                l2l.data.transforms.ConsecutiveLabels(meta_data),\r\n",
        "                                        ],\r\n",
        "                                        num_tasks=len(LOSO_dataset.keys())) \r\n",
        "    #인원수에 맞추어 테스크의 수를 세팅\r\n",
        "    \r\n",
        "\r\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "    model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\r\n",
        "    meta_model = l2l.algorithms.MAML(model, lr=maml_lr, first_order=False)\r\n",
        "\r\n",
        "    opt = optim.Adam(meta_model.parameters(), lr=lr)\r\n",
        "    loss_func = nn.CrossEntropyLoss(reduction='mean')\r\n",
        "    with torch.backends.cudnn.flags(enabled=False):\r\n",
        "\r\n",
        "        for iteration in range(iterations):\r\n",
        "            \r\n",
        "            iteration_error = 0.0\r\n",
        "            iteration_acc = 0.0\r\n",
        "            iteration_f1 = 0.0\r\n",
        "            for _ in range(tps):\r\n",
        "                learner = meta_model.clone()\r\n",
        "                train_task = train_tasks.sample()\r\n",
        "                data, labels = train_task\r\n",
        "                data = data.to(device)\r\n",
        "                labels = labels.to(device)\r\n",
        "\r\n",
        "                # Separate data into adaptation/evalutation sets\r\n",
        "                adaptation_indices = np.zeros(data.size(0), dtype=bool)\r\n",
        "                adaptation_indices[np.arange(shots*ways) * 2] = True\r\n",
        "                evaluation_indices = torch.from_numpy(~adaptation_indices)\r\n",
        "                adaptation_indices = torch.from_numpy(adaptation_indices)\r\n",
        "                adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\r\n",
        "                evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\r\n",
        "\r\n",
        "                # Fast Adaptation\r\n",
        "                for step in range(fas):\r\n",
        "                    train_error = loss_func(learner(adaptation_data), adaptation_labels)\r\n",
        "                    learner.adapt(train_error)\r\n",
        "\r\n",
        "                # Compute validation loss\r\n",
        "                predictions = learner(evaluation_data)\r\n",
        "\r\n",
        "                valid_error = loss_func(predictions, evaluation_labels)\r\n",
        "                valid_error /= len(evaluation_data)\r\n",
        "                predictions = predictions.argmax(dim=1)\r\n",
        "                valid_accuracy = accuracy_score(predictions.cpu(), evaluation_labels.cpu())\r\n",
        "                valid_f1 = f1_score(predictions.cpu(), evaluation_labels.cpu())\r\n",
        "                iteration_error += valid_error\r\n",
        "                iteration_acc += valid_accuracy\r\n",
        "                iteration_f1 += valid_f1\r\n",
        "\r\n",
        "            iteration_error /= tps\r\n",
        "            iteration_acc /= tps\r\n",
        "            iteration_f1 /= tps\r\n",
        "            print('Iteration: {} Loss : {:.3f} Acc : {:.3f} F1 : {:.3f}'.format(iteration, iteration_error.item(), iteration_acc, iteration_f1))\r\n",
        "\r\n",
        "            # Take the meta-learning step\r\n",
        "            opt.zero_grad()\r\n",
        "            iteration_error.backward()\r\n",
        "            opt.step()\r\n",
        "    PATH = '/content/drive/My Drive/KEMOC/META_MODEL/AROUSAL/'\r\n",
        "    torch.save(meta_model, PATH + 'model_'+ID+'.pt')  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:390: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if param.grad is not None:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0 Loss : 0.173 Acc : 0.500 F1 : 0.000\n",
            "Iteration: 1 Loss : 0.173 Acc : 0.500 F1 : 0.667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyAMOn60YD4z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}